# About This Example

This example demonstrates a case where simply telling the agent to "go fix" wasn't enough. I wanted to add automated tests for my newly created PowerShell installation script using Pester. Test cases were already defined in the backlog item, so I included them from the start. The initial test implementation went smoothly, but when I ran the tests, not all of them passed. This is fairly common when having Copilot implement anything for the first time—it typically sees the failed tests and attempts to fix them. However, in this case, it got stuck in a loop, repeatedly trying to fix missing mock references and changing imports. [Implement and fix tests](1.Implement%20pester%20tests.md)

After several stubborn attempts at the "go fix" approach, it became clear that the agent needed a better plan—and probably a fresh context—to resolve the issues. I tried having it analyze the failed tests and create a plan to fix them, this time including Pester's mocking documentation. [Create plan to fix](2.Better-test-fix-plan.md) Unfortunately, it soon fell back into the same loop of trying to fix function mocking and changing imports.

At this point, it was time to roll up my sleeves and dig into the details myself. My suspicion was that the module dependencies were incorrect. Given that this code had gone through several architecture changes and refactorings, it was likely that the dependencies had become suboptimal. I directed the agent to the `Import-Module` documentation and Pester's mocking documentation, then asked it to investigate how our current dependency management compared to documented best practices. [Analyse dependency management](3.Analyse%20dependency%20mangement.md)

It turned out that the import statements had become quite messy, but the solution was straightforward: use module manifests to manage dependencies and clean up the import statements in the scripts. After implementing these changes, all the missing mock issues were resolved, and I was back to dealing with only functional test failures. 

## Why Didn't This Work?
- The agent lacked the context needed to consider the dependencies of the modules being mocked.
- The agent required guidance to relevant documentation to understand best practices.
- Training data likely contains many examples of poor module dependency practices, making explicit documentation references essential.
- PowerShell module dependencies are notoriously difficult to get right, even for experienced developers.

## Key Takeaways
- Even though using Copilot initially caused frustration, it ultimately helped me understand and resolve the dependency management issues.
- Endless "go fix" loops are an excellent way to burn through your premium request limit without actually fixing anything. Always try to understand the root cause and provide the agent with better context and a clear plan to address it.